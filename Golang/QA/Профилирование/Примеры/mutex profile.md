### Получение и анализ профиля мьютексов (mutex profile)
Этот профиль показывает, в каких частях кода горутины дольше всего ждут освобождения мьютексов.

### Шаг 1: Включение сбора данных

В отличие от профилей CPU или памяти, сбор данных о состязаниях за мьютексы **отключен по умолчанию**. Чтобы его включить, нужно в самом начале работы программы (в идеале, в функции `main`) вызвать `runtime.SetMutexProfileFraction`.

Эта функция принимает целое число `n`. Если `n > 0`, то будет собираться информация о `1/n` всех событий состязания за мьютекс.
- `runtime.SetMutexProfileFraction(1)`: Собирать данные обо **всех** событиях. Максимально точно, но может немного замедлить программу.
- `runtime.SetMutexProfileFraction(5)`: Собирать данные о каждом пятом событии (20%). Хороший компромисс между точностью и производительностью.
    

### Шаг 2: Пример кода и получение профиля

Давайте создадим программу, которая намеренно вызывает борьбу за мьютекс, и получим её профиль.

#### Пример кода (`main.go`)

Этот код запускает 1000 горутин, которые пытаются одновременно записать данные в общую карту (`map`), защищённую одним мьютексом.


```go
package main

import (
	"fmt"
	"net/http"
	_ "net/http/pprof" // Импорт для регистрации обработчиков pprof
	"runtime"
	"sync"
)

// Общий ресурс, за который будут бороться горутины
var sharedMap = make(map[int]int)
var mu sync.Mutex

// Функция, которая интенсивно использует мьютекс
func writeToMap(wg *sync.WaitGroup, i int) {
	defer wg.Done()

	mu.Lock() // Блокируем доступ
	defer mu.Unlock()

	// Имитация какой-то работы под мьютексом
	sharedMap[i] = i * 2
}

func main() {
	// ВАЖНО: Включаем профилирование мьютексов.
	// Будем собирать данные о каждой 5-й блокировке.
	runtime.SetMutexProfileFraction(5)

	// Запускаем веб-сервер для pprof
	go func() {
		fmt.Println("Starting pprof server on :6060")
		// Адрес для профиля мьютексов: http://localhost:6060/debug/pprof/mutex
		http.ListenAndServe("localhost:6060", nil)
	}()

	var wg sync.WaitGroup
	// Запускаем множество горутин для создания состязания
	for i := 0; i < 1000; i++ {
		wg.Add(1)
		go writeToMap(&wg, i)
	}

	wg.Wait()
	fmt.Println("All goroutines finished. Map size:", len(sharedMap))
}
```

#### Получение профиля

1. **Запустите программу:**
    
    ```bash
    go run main.go
    ```
    
    Программа выполнится и завершится, но pprof-сервер продолжит работать. Если бы это было реальное приложение, оно бы работало постоянно.
    
2. **Запросите профиль мьютексов:** Используйте `go tool pprof`, указав URL профиля `/debug/pprof/mutex`.
```bash
    go tool pprof http://localhost:6060/debug/pprof/mutex
```
    
Эта команда скачает профиль и откроет интерактивную консоль `pprof`.
    

### Шаг 3: Анализ профиля мьютексов

В консоли `pprof` вы можете использовать те же команды, что и для других профилей, но их вывод будет означать другое.

- **Значения в профиле:** `pprof` покажет **совокупное время ожидания** на блокировках, а не потребление CPU или памяти.
    

#### **Основные команды для анализа**

1. **`top` — найти главные точки состязания** Эта команда покажет функции, в которых горутины провели больше всего времени, ожидая мьютекс.
    
    ```bash
    (pprof) top
    Showing nodes accounting for 6.45ms, 100% of 6.45ms total
          flat  flat%   sum%        cum   cum%
        6.45ms   100%   100%     6.45ms   100%  main.writeToMap
             0     0%   100%     6.45ms   100%  main.main.func2
    ```
    
    - **`flat`**: Время, которое горутины провели в ожидании мьютекса _непосредственно в этой функции_.
        
    - **`cum` (cumulative)**: Суммарное время ожидания в этой функции и во всех, которые она вызывала.
        
    
    Здесь мы видим, что почти всё время ожидания (6.45ms) пришлось на нашу функцию `main.writeToMap`.
    
2. **`list <имя_функции>` — найти конкретную строку** Команда `list` покажет исходный код функции и укажет на строку, где происходит блокировка.
    
    ```bash
    (pprof) list writeToMap
    Total: 6.45ms
    ROUTINE ======================== main.writeToMap in /path/to/project/main.go
         6.45ms     6.45ms (flat, cum)   100% of Total
              .          .     16:
              .          .     17: func writeToMap(wg *sync.WaitGroup, i int) {
              .          .     18:   defer wg.Done()
              .          .     19:
         6.45ms     6.45ms     20:   mu.Lock() // Блокируем доступ
              .          .     21:   defer mu.Unlock()
              .          .     22:
              .          .     23:   // Имитация какой-то работы под мьютексом
              .          .     24:   sharedMap[i] = i * 2
              .          .     25: }
    ```
    
    Анализ `list` не оставляет сомнений: строка `mu.Lock()` — это именно то место, где горутины "толпятся" в ожидании.
    
3. **`web` — визуализировать проблему** Эта команда сгенерирует граф вызовов (требуется установленный Graphviz), где самые "горячие" по времени ожидания блокировки будут выделены. Это особенно полезно в сложных системах, чтобы понять, какой путь вызовов приводит к проблемной блокировке.
    

**Вывод анализа:** Проблема в том, что слишком много горутин пытаются получить доступ к одному и тому же ресурсу через один мьютекс. Возможные решения могли бы включать использование `sync.Map`, разделение данных на несколько сегментов со своими мьютексами или пересмотр архитектуры для уменьшения необходимости в общей блокировке.