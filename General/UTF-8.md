**Кодовая точка UTF-8** — это числовое представление символа в компьютере. Но как именно это число преобразуется в последовательность байтов, которую понимает компьютер? Давайте разберемся.

### Основные принципы UTF-8:

- **Переменная длина:** Длина кода символа в байтах зависит от его кодовой точки. Более распространенные символы (например, латинские буквы) кодируются меньшим количеством байтов, чем редкие символы (например, иероглифы).
- **Байты-маркеры:** Каждый байт в коде символа содержит информацию о том, является ли он первым, средним или последним в последовательности. Это позволяет однозначно определять границы символов.
- **Совместимость с ASCII:** Первые 128 кодовых точек UTF-8 совпадают с кодировкой ASCII, что обеспечивает совместимость с более старыми системами.

### Процесс формирования кодовой точки UTF-8:

1. **Определение длины кода:**
    
    - Для кодовых точек до U+007F (127) используется 1 байт.
    - Для кодовых точек от U+0080 до U+07FF используется 2 байта.
    - И так далее, для более высоких кодовых точек используется 3, 4, 5 или 6 байтов.
2. **Распределение битов:**
    
    - Каждый байт делится на два типа битов: старшие и младшие. Старшие биты определяют длину кода, а младшие содержат часть кодовой точки.
    - Первый байт содержит информацию о длине кода и старшие биты кодовой точки.
    - Остальные байты (если они есть) начинаются с определенной последовательности бит (10xxxxxx), чтобы указать, что они являются частью многобайтового кода.
3. **Заполнение байтов:**
    
    - Младшие биты каждого байта заполняются оставшимися битами кодовой точки.

### Пример:

Предположим, мы хотим закодировать символ "€" (евро), который имеет кодовую точку U+20AC (в шестнадцатеричном виде).

1. Определяем длину кода: U+20AC находится в диапазоне от U+0800 до U+FFFF, поэтому для кодирования потребуется 3 байта.
2. Делим кодовую точку на части:
    - 11100010 (старшие биты первого байта)
    - 10000010 (второй байт)
    - 10101100 (третий байт)
3. Формируем окончательный код:
    - 11100010 10000010 10101100

В шестнадцатеричном виде это будет E2 82 AC.

|Количество байтов |Старшие биты первого байта |
|---|---|
|1 |0xxx xxxx |
|2 |110x xxxx |
|3 |1110 xxxx |
|4 |1111 0xxx |

### Почему так сложно?

- **Универсальность:** UTF-8 должен уметь кодировать огромное количество символов, поэтому требуется гибкая система кодирования.
- **Совместимость:** UTF-8 должен быть совместим с существующими кодировками, такими как ASCII.