
### Зачем нужна асимптотическая сложность?

- **Сравнение алгоритмов:** Позволяет объективно сравнивать эффективность разных алгоритмов для решения одной и той же задачи. Например, алгоритм с O(N2) будет работать намного медленнее, чем алгоритм с O(N log N) на больших данных.
    
- **Прогнозирование производительности:** Помогает предсказать, как будет вести себя алгоритм на очень больших массивах данных.


### Основные обозначения (нотации)

- **O-большое (Big O notation)**:  Описывает **наихудший случай** или **верхнюю границу** роста. Например, O(N2) означает, что время выполнения **никогда не превысит** c⋅N2 для некоторой константы c при достаточно большом N. Это наиболее часто используемая нотация.
    
- **Ω-большое (Big Omega notation)**: Описывает **наилучший случай** или **нижнюю границу**. Например, Ω(N) означает, что время выполнения **всегда будет не меньше**, чем c⋅N.
    
- **Θ-большое (Big Theta notation)**:  Описывает **точную асимптотическую границу**. Например, Θ(N) означает, что время выполнения находится как в O(N), так и в Ω(N), то есть его рост **линейно** зависит от N. Приблизительно можно сказать, что тета описывает средний случай.

### Примеры асимптотической сложности

- **O(1)** (Константная): Время выполнения не зависит от размера данных (например, доступ к элементу массива по индексу).
    
- **O(log N)** (Логарифмическая): Очень быстрый рост, часто встречается в алгоритмах, которые делят задачу на части (например, бинарный поиск).
    
- **O(N)** (Линейная): Время выполнения пропорционально размеру данных (например, поиск элемента в несортированном списке).
    
- **O(N log N)** (Линейно-логарифмическая): Часто встречается в эффективных алгоритмах сортировки (например, Merge Sort, Quick Sort).
    
- **O(N^2)** (Квадратичная): Медленный рост, часто связан с вложенными циклами (например, простая сортировка пузырьком).


![[07074817012022_6caf85fa09e0642959e62c753d9a2f18236eb1da.jpg]]